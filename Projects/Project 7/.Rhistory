# parallel trends: add a third state
# ----------------------------------------------------------------
medicaid_expansion %>%
# process
# ---------
filter(State %in% c("Hawaii", "Pennsylvania")) %>%
# north dakota, kentucky, nevada, arizona, Date_Adopted %in% c("2016-01-01") |
# plot
# ---------
ggplot() +
geom_point(aes(x = year,
y = uninsured_rate,
color = State)) +
geom_line(aes(x = year,
y = uninsured_rate,
color = State)) +
geom_vline(xintercept = 2014, color = "maroon") +
geom_vline(xintercept = 2016, color = "darkgreen")
# themes
theme_fivethirtyeight() +
theme(axis.title = element_text()) +
# labels
ggtitle('States Enrolled on Jan 1, 2014 and Jan 1, 2016, \nbefore/after Enrollment') +
xlab('Year') +
ylab('Uninsured Rate')
# Difference-in-Differences estimation
# create a dataset for kansas and colorado
HI_PA <-
medicaid_expansion %>%
filter(State %in% c("Pennsylvania","Hawaii")) %>%
filter(year >= 2014 & year <= 2015)
# pre-treatment difference
# ----------
pre_diff <-
HI_PA %>%
# filter out only the year we want
filter(year == 2014) %>%
# subset to select only vars we want
select(State,
uninsured_rate) %>%
# make the data wide
pivot_wider(names_from = State,
values_from = uninsured_rate) %>%
# subtract to make calculation
summarise(Pennsylvania - Hawaii)
# post-treatment difference
# ----------
post_diff <-
HI_PA %>%
# filter out only the quarter we want
filter(year == 2015) %>%
# subset to select only vars we want
select(State,
uninsured_rate) %>%
# make the data wide
pivot_wider(names_from = State,
values_from = uninsured_rate) %>%
# subtract to make calculation
summarise(Pennsylvania - Hawaii)
# diff-in-diffs
# ----------
diff_in_diffs <- post_diff - pre_diff
diff_in_diffs
# chunk settings
# ----------
knitr::opts_chunk$set(message = FALSE, warning = FALSE, warning = FALSE)
# Install packages
# ----------
if (!require("pacman")) install.packages("pacman")
pacman::p_load(# Tidyverse packages including dplyr and ggplot2
tidyverse,
ggthemes,
ltmle,   # Longitudinal Targeted Maximum Likelihood Estimation
tmle,    # Targeted Maximum Likelihood Estimation
SuperLearner,
caret,
e1071,       # svm
rpart,       # decision trees
furrr,       # parallel processing
parallel,    # parallel processing
ranger,      # fast implementation of random forests
tidymodels)  # workflow for machine learning
# set seed
# ----------
set.seed(44)
# Load Boston dataset from MASS package
# ----------
data(Boston, package = "MASS")
glimpse(Boston)
# initial split
# ----------
boston_split <-
initial_split(Boston, prop = 3/4) # create initial split (tidymodels)
# Training
# ----------
train <-
# Declare the training set with rsample::training()
training(boston_split)
# y_train
y_train <-
train %>%
# is medv where medv > 22 is a 1, 0 otherwise
mutate(medv = ifelse(medv > 22,
1,
0)) %>%
# pull and save as vector
pull(medv)
# x_train
x_train <-
train %>%
# drop the target variable
select(-medv)
# Testing
# ----------
test <-
# Declare the training set with rsample::training()
testing(boston_split)
# y test
y_test <-
test %>%
mutate(medv = ifelse(medv > 22,
1,
0)) %>%
pull(medv)
# x test
x_test <-
test %>%
select(-medv)
# initial split
# ----------
boston_split <-
initial_split(Boston, prop = 3/4) # create initial split (tidymodels)
# Training
# ----------
train <-
# Declare the training set with rsample::training()
training(boston_split)
# y_train
y_train <-
train %>%
# is medv where medv > 22 is a 1, 0 otherwise
mutate(medv = ifelse(medv > 22,
1,
0)) %>%
# pull and save as vector
pull(medv)
# x_train
x_train <-
train %>%
# drop the target variable
select(-medv)
# Testing
# ----------
test <-
# Declare the training set with rsample::training()
testing(boston_split)
# y test
y_test <-
test %>%
mutate(medv = ifelse(medv > 22,
1,
0)) %>%
pull(medv)
# x test
x_test <-
test %>%
select(-medv)
# Superlearner:  list all the machine learning algorithms
# ----------
listWrappers()
# set seed
set.seed(12345)
# LASSO
# ----------
sl_lasso <- SuperLearner(Y = y_train,              # target
X = x_train,              # features
family = binomial(),      # binomial : 1,0s
SL.library = "SL.glmnet") # find the glmnet algo from SL
# view
sl_lasso
# set seed
set.seed(12345)
# LASSO
# ----------
sl_lasso <- SuperLearner(Y = y_train,              # target
X = x_train,              # features
family = binomial(),      # binomial : 1,0s
SL.library = "SL.glmnet") # find the glmnet algo from SL
# view
sl_lasso
# set seed
set.seed(12345)
# LASSO
# ----------
sl_lasso <- SuperLearner(Y = y_train,              # target
X = x_train,              # features
family = binomial(),      # binomial : 1,0s
SL.library = "SL.glmnet") # find the glmnet algo from SL
# view
sl_lasso
# Here is the risk of the best model (discrete SuperLearner winner).
# Use which.min boolean to find minimum cvRisk in list
sl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]
sl_lass$cvRisk
sl_lasso$cvRisk
# Here is the risk of the best model (discrete SuperLearner winner).
# Use which.min boolean to find minimum cvRisk in list
sl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]
sl_lasso$cvRisk
# set seed
set.seed(987)
# multiple models
# ----------
sl = SuperLearner(Y = y_train,
X = x_train,
family = binomial(),
# notice these models are concatenated
SL.library = c('SL.mean',    # if you just guessed the average - serves as a baseline
'SL.glmnet',
'SL.ranger'))
sl
# predictions
# ----------
preds <-
predict(sl,             # use the superlearner not individual models
x_test,         # prediction on test set
onlySL = TRUE)  # use only models that were found to be useful (had weights)
# start with y_test
validation <-
y_test %>%
# add our predictions - first column of predictions
bind_cols(preds$pred[,1]) %>%
# rename columns
rename(obs = `...1`,      # actual observations
pred = `...2`) %>% # predicted prob
# change pred column so that obs above .5 are 1, otherwise 0
mutate(pred = ifelse(pred >= .5,
1,
0))
# view
head(validation)
# confusion matrix
# ----------
caret::confusionMatrix(as.factor(validation$pred),
as.factor(validation$obs))
#
# Macs/Linux
# --------------------------------------------------------
# identify number of cores to use
n_cores <- availableCores() - 1 # maybe 2 cores instead?
# plan separate session
plan(multisession,
workers = n_cores)
# set seed - option will set same seed across multiple cores
set.seed(44, "L'Ecuyer-CMRG")
# cross-validation across cores
# ----------
cv_sl = CV.SuperLearner(Y = y_train,
X = x_train,
family = binomial(),      #
V = 20,                   # default fold recommendation for Superlearner
parallel = 'multicore',   # macs linux parallel specification, note its a string
SL.library = c("SL.mean",
"SL.glmnet",
"SL.ranger"))
# plot
plot(cv_sl)
#
# Macs/Linux
# --------------------------------------------------------
# identify number of cores to use
n_cores <- availableCores() - 2 # maybe 2 cores instead?
# plan separate session
plan(multisession,
workers = n_cores)
# set seed - option will set same seed across multiple cores
set.seed(44, "L'Ecuyer-CMRG")
# cross-validation across cores
# ----------
cv_sl = CV.SuperLearner(Y = y_train,
X = x_train,
family = binomial(),      #
V = 20,                   # default fold recommendation for Superlearner
parallel = 'multicore',   # macs linux parallel specification, note its a string
SL.library = c("SL.mean",
"SL.glmnet",
"SL.ranger"))
# plot
plot(cv_sl)
# specify which SuperLearner algorithms we want to use
# ----------
sl_libs <- c('SL.glmnet', 'SL.ranger', 'SL.glm')
# Prepare data for SuperLearner/TMLE
# ----------
# Mutate Y, A for outcome and treatment, use tax, age, and crim as covariates
data_obs <-
Boston %>%
mutate(Y = ifelse(medv > 22,
1,
0)) %>%
rename(A = chas) %>%
select(Y, A, tax, age, crim)  # A = borders the charles river
# Outcome
# ----------
Y <-
data_obs %>%
pull(Y)
# Covariates
# ----------
W_A <-
data_obs %>%
select(-Y)
# Fit SL for Q step, initial estimate of the outcome
# ----------
Q <- SuperLearner(Y = Y,                # outcome
X = W_A,              # covariates + treatment
family = binomial(),  # binominal bc outcome is binary
SL.library = sl_libs) # ML algorithms
# observed treatment
# ----------
Q_A <- as.vector(predict(Q)$pred)
# if every unit was treated (pretending every unit was treated)
# ----------
W_A1 <- W_A %>% mutate(A = 1)
Q_1 <- as.vector(predict(Q, newdata = W_A1)$pred)
# if everyone was control (pretending every unit was not treated)
# ----------
W_A0 <- W_A %>% mutate(A = 0)
Q_0 <- as.vector(predict(Q, newdata = W_A0)$pred)
# observed treatment
# ----------
Q_A <- as.vector(predict(Q)$pred)
# if every unit was treated (pretending every unit was treated)
# ----------
W_A1 <- W_A %>% mutate(A = 1)
Q_1 <- as.vector(predict(Q, newdata = W_A1)$pred)
# if everyone was control (pretending every unit was not treated)
# ----------
W_A0 <- W_A %>% mutate(A = 0)
Q_0 <- as.vector(predict(Q, newdata = W_A0)$pred)
# combine all predictions into one dataframe
# ----------
dat_tmle <- tibble(Y = Y,
A = W_A$A,
Q_A,
Q_0,
Q_1)
# view
head(dat_tmle)
# view
head(dat_tmle, n=20)
# view
head(dat_tmle, n=40)
# view
head(dat_tmle)
# G-computation
# ----------
ate_gcomp <- mean(dat_tmle$Q_1 - dat_tmle$Q_0)
ate_gcomp
A
# prepare data for analysis
# ----------
A <- W_A$A
W <- Boston %>% select(tax, age, crim)  # select jsut a few covariates
# model probability of treatment (similar to matching)
# ----------
g <- SuperLearner(Y = A,              # outcome is treatment
X = W,              # vector of covariates (predictors)
family=binomial(),  # binary outcome
SL.library=sl_libs) # models
A
A
# prepare data for analysis
# ----------
A <- W_A$A
W <- Boston %>% select(tax, age, crim)  # select jsut a few covariates
# model probability of treatment (similar to matching)
# ----------
g <- SuperLearner(Y = A,              # outcome is treatment
X = W,              # vector of covariates (predictors)
family=binomial(),  # binary outcome
SL.library=sl_libs) # models
# Prediction for probability of treatment
# ----------
g_w <- as.vector(predict(g)$pred) # Pr(A=1|W)
# probability of treatment: iptw : (inverse probability weight of receiving treatment)
# ----------
H_1 <- 1/g_w
# probability of control: niptw : (negative inverse probability weight of not receiving treatment)
# ----------
H_0 <- -1/(1-g_w)
# create a clever covariate
# ----------
dat_tmle <- # add clever covariate data to dat_tmle
dat_tmle %>%
bind_cols(
H_1 = H_1,
H_0 = H_0) %>%
# create clever covariate whereby case takes iptw or niptw based on treatment status
mutate(H_A = case_when(A == 1 ~ H_1,
A == 0 ~ H_0))
# fluctuation parameter
# ----------
glm_fit <- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A,
data=dat_tmle,
family=binomial)
# fluctuation parameter
# ----------
glm_fit <- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A,
data=dat_tmle,
family=binomial)
# information we need to summarize probability of treatment step above -- how much to update
# ----------
eps <- coef(glm_fit)
# add H_A variable
# ----------
H_A <- dat_tmle$H_A
# predictions for treatment status
# ----------
Q_A_update <- plogis(qlogis(Q_A) + eps*H_A) # updating by adding eps * H_A
# add H_A variable
# ----------
H_A <- dat_tmle$H_A
# predictions for treatment status
# ----------
Q_A_update <- plogis(qlogis(Q_A) + eps*H_A) # updating by adding eps * H_A
# predictions assuming everyone under treatment
# ----------
Q_1_update <- plogis(qlogis(Q_1) + eps*H_1) # updating by adding eps * H_1
# predictions assuming everyone under  control
# ----------
Q_0_update <- plogis(qlogis(Q_0) + eps*H_0) # updating by adding eps * H_0
# calculate ATE
# ----------
tmle_ate <- mean(Q_1_update - Q_0_update)
tmle_ate
# calculate standard errors and p-values
# ----------
infl_fn <- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate
# calculate ATE
# ----------
tmle_se <- sqrt(var(infl_fn)/nrow(data_obs))
# confidence intervals
conf_low <- tmle_ate - 1.96*tmle_se
conf_high <- tmle_ate + 1.96*tmle_se
# p-values
pval <- 2 * (1 - pnorm(abs(tmle_ate / tmle_se)))
# view
tmle_ate
conf_low
conf_high
pval
# set seed for reproducibility
set.seed(1000)
# implement above all in one step using tmle
# ----------
tmle_fit <-
tmle::tmle(Y = Y,                  # outcome
A = A,                  # treatment
W = W,                  # baseline covariates
Q.SL.library = sl_libs, # libraries for initial estimate
g.SL.library = sl_libs) # libraries for prob to be in treatment
# view results
tmle_fit
# process data
# ----------
data_obs_ltmle <-
data_obs %>%
# need to specify W1, W2, etc
rename(W1 = tax, W2 = age, W3 = crim) %>%
select(W1, W2, W3, A, Y)
# implement ltmle
# ----------
result <- ltmle(data_obs_ltmle, # dataset
Anodes = "A",   # vector that shows treatment
Ynodes = "Y",   # vector that shows outcome
abar = 1)
# view
result
treatment
# Install packages
if (!require("pacman")) install.packages("pacman")
# We are using a package (augsynth) that is not on CRAN, R packages on CRAN have to pass
# some formal tests. Always proceed with caution if a packages is not on CRAN. Since the
# R package is not on CRAN, we needed to download and install the package directly from
# GitHub. Always use the CRAN version if there is one because it is most stable. However,
# if you need something that is currently in development, you might want to download from
# GitHub. I've commented out the workflow since I already have it on my computer:
#
# workflow to install a package from GitHub
# ----------------------------------------------------------------
# 1. install `devtools` if you don't already have it. Note that you might need to update the 'rlang' package
# ----------
install.packages("devtools") # download developer tools package
library(devtools)            # load library
# 2. install the package ("augsynth"). you can find this path on the GitHub instructions
# ----------
devtools::install_github("ebenmichael/augsynth")
# install libraries - install "augsynth" here since it is now on CRAN
pacman::p_load(# Tidyverse packages including dplyr and ggplot2
tidyverse,
ggthemes,
augsynth)
#
# chunk options
# ----------------------------------------------------------------
knitr::opts_chunk$set(
warning = FALSE            # prevents warning from appearing after code chunk
)
# set seed
set.seed(44)
install.packages("devtools")
